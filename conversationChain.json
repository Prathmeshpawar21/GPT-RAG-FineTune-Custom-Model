{"name":null,"memory":"ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[]), return_messages=True, memory_key='chat_history')","callbacks":null,"verbose":false,"tags":null,"metadata":null,"callback_manager":null,"combine_docs_chain":"StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=HuggingFaceEndpoint(repo_id='mistralai/Mistral-Nemo-Instruct-2407', temperature=0.4, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 128, 'token': 'hf_VgKhuKyfBOiyfyjXfdsadgdsvdfvuH'}, model='mistralai/Mistral-Nemo-Instruct-2407', client=<InferenceClient(model='mistralai/Mistral-Nemo-Instruct-2407', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-Nemo-Instruct-2407', timeout=120)>), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context')","question_generator":"LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=HuggingFaceEndpoint(repo_id='mistralai/Mistral-Nemo-Instruct-2407', temperature=0.4, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 128, 'token': 'hf_VgKhuKyfBOiyfyjXfdsadgdsvdfvuH'}, model='mistralai/Mistral-Nemo-Instruct-2407', client=<InferenceClient(model='mistralai/Mistral-Nemo-Instruct-2407', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-Nemo-Instruct-2407', timeout=120)>), output_parser=StrOutputParser(), llm_kwargs={})","output_key":"answer","rephrase_question":true,"return_source_documents":false,"return_generated_question":false,"get_chat_history":null,"response_if_no_docs_found":null,"retriever":"VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001BC03966990>, search_kwargs={})","max_tokens_limit":null}